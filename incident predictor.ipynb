{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5a3176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incident Occurred Predictor - avoiding data pollution\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc06666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Read CSV\n",
    "csv_path = \"avalon_nuclear.csv\"\n",
    "if not os.path.exists(csv_path):\n",
    "    raise FileNotFoundError(f\"{csv_path} not found in working directory.\")\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Loaded:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "# Check incident_occurred distribution\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Target Variable Distribution: incident_occurred\")\n",
    "print(\"=\"*70)\n",
    "print(df['incident_occurred'].value_counts())\n",
    "print(f\"\\nIncident Rate: {df['incident_occurred'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c015e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seaborn style for beautiful plots\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"husl\")\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "\n",
    "# Select only numerical columns for correlation analysis\n",
    "numerical_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate correlations with incident_occurred\n",
    "correlations = numerical_df.corr()['incident_occurred'].sort_values(ascending=False)\n",
    "\n",
    "# Remove incident_occurred itself from the list\n",
    "correlations = correlations.drop('incident_occurred')\n",
    "\n",
    "# Create a beautiful horizontal bar plot using seaborn\n",
    "plt.close('all')  # Close previous figures to prevent lag\n",
    "fig, ax = plt.subplots(figsize=(12, 14))\n",
    "\n",
    "# Create color palette\n",
    "colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in correlations.values]\n",
    "\n",
    "# Use seaborn barplot\n",
    "sns.barplot(x=correlations.values, y=correlations.index, palette=colors, ax=ax, \n",
    "            edgecolor='black', linewidth=0.5, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Correlation Coefficient', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('')\n",
    "ax.set_title('Feature Correlation with Incident Occurred', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=1.5)\n",
    "sns.despine(top=True, right=True)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, val in enumerate(correlations.values):\n",
    "    if abs(val) > 0.05:  # Only label significant correlations\n",
    "        ax.text(val + (0.01 if val > 0 else -0.01), i, f'{val:.3f}', \n",
    "                va='center', ha='left' if val > 0 else 'right', fontsize=8, fontweight='bold')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='#2ecc71', alpha=0.8, label='Positive Correlation'),\n",
    "                   Patch(facecolor='#e74c3c', alpha=0.8, label='Negative Correlation')]\n",
    "ax.legend(handles=legend_elements, loc='lower right', fontsize=10, framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top correlations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 10 POSITIVE CORRELATIONS WITH INCIDENT_OCCURRED\")\n",
    "print(\"=\"*60)\n",
    "print(correlations.head(10))\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 10 NEGATIVE CORRELATIONS WITH INCIDENT_OCCURRED\")\n",
    "print(\"=\"*60)\n",
    "print(correlations.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eqf3og1xpaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling: 60% train, 20% validation, 20% test split\n",
    "# CRITICAL: Drop avalon_shutdown_recommendation and true_risk_level to avoid data pollution\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸš¨ DROPPING COLUMNS TO PREVENT DATA POLLUTION ðŸš¨\")\n",
    "print(\"=\"*70)\n",
    "print(\"Dropping: avalon_shutdown_recommendation, true_risk_level\")\n",
    "print(\"Reason: These are derived/target variables that would leak information\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Separate features and target\n",
    "columns_to_drop = ['incident_occurred', 'avalon_shutdown_recommendation', 'true_risk_level']\n",
    "X = df.drop(columns_to_drop, axis=1)\n",
    "y = df['incident_occurred']\n",
    "\n",
    "print(f\"\\nOriginal features: {df.shape[1]}\")\n",
    "print(f\"Features after dropping pollution columns: {X.shape[1]}\")\n",
    "print(f\"\\nRemaining features: {list(X.columns)}\")\n",
    "\n",
    "# Handle categorical variables (country) - encode it\n",
    "le = LabelEncoder()\n",
    "X_processed = X.copy()\n",
    "if 'country' in X_processed.columns:\n",
    "    X_processed['country'] = le.fit_transform(X_processed['country'])\n",
    "\n",
    "# First split: 60% train, 40% temp (which will be split into 20% val, 20% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_processed, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: split the 40% into 50-50 (20% val, 20% test of original)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train)} ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation set size: {len(X_val)} ({len(X_val)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(X_test)} ({len(X_test)/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nTarget distribution in training set:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "\n",
    "# Try different normalization techniques\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"     ðŸ”¬ FEATURE NORMALIZATION COMPARISON ðŸ”¬\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Standard Scaler (Z-score normalization)\n",
    "scaler_standard = StandardScaler()\n",
    "X_train_standard = scaler_standard.fit_transform(X_train)\n",
    "X_val_standard = scaler_standard.transform(X_val)\n",
    "X_test_standard = scaler_standard.transform(X_test)\n",
    "\n",
    "# 2. MinMax Scaler (scales to [0, 1])\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_minmax = scaler_minmax.fit_transform(X_train)\n",
    "X_val_minmax = scaler_minmax.transform(X_val)\n",
    "X_test_minmax = scaler_minmax.transform(X_test)\n",
    "\n",
    "# 3. Robust Scaler (uses median and IQR, robust to outliers)\n",
    "scaler_robust = RobustScaler()\n",
    "X_train_robust = scaler_robust.fit_transform(X_train)\n",
    "X_val_robust = scaler_robust.transform(X_val)\n",
    "X_test_robust = scaler_robust.transform(X_test)\n",
    "\n",
    "# Store for backwards compatibility\n",
    "X_train_scaled = X_train_standard\n",
    "X_val_scaled = X_val_standard\n",
    "X_test_scaled = X_test_standard\n",
    "\n",
    "print(\"\\nâœ… Created three normalized versions:\")\n",
    "print(\"   1. StandardScaler (Z-score): Mean=0, Std=1\")\n",
    "print(\"   2. MinMaxScaler: Scaled to [0, 1]\")\n",
    "print(\"   3. RobustScaler: Median-centered, robust to outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q5slmn75o4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with Lasso (L1) regularization for binary classification\n",
    "# Test different lambda (C) values - note: sklearn uses C = 1/lambda\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, f1_score\n",
    "\n",
    "# Range of C values (inverse of lambda)\n",
    "# Smaller C = stronger regularization (larger lambda)\n",
    "C_values = np.logspace(-3, 3, 50)  # From 0.001 to 1000\n",
    "lambdas = 1 / C_values  # Convert to lambda for interpretation\n",
    "\n",
    "# Store coefficients for each C value\n",
    "coefficients = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "val_f1_scores = []\n",
    "val_auc_scores = []\n",
    "\n",
    "print(\"Training Lasso Logistic Regression models with varying regularization...\")\n",
    "\n",
    "for C in C_values:\n",
    "    # Train logistic regression with L1 penalty (Lasso)\n",
    "    model = LogisticRegression(\n",
    "        penalty='l1',\n",
    "        C=C,\n",
    "        solver='saga',  # saga solver supports L1 penalty\n",
    "        max_iter=2000,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Store coefficients\n",
    "    coefficients.append(np.abs(model.coef_[0]))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train_scaled))\n",
    "    val_acc = accuracy_score(y_val, model.predict(X_val_scaled))\n",
    "    val_f1 = f1_score(y_val, model.predict(X_val_scaled))\n",
    "    \n",
    "    # Get probability predictions for AUC\n",
    "    val_probs = model.predict_proba(X_val_scaled)[:, 1]\n",
    "    val_auc = roc_auc_score(y_val, val_probs)\n",
    "    \n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    val_f1_scores.append(val_f1)\n",
    "    val_auc_scores.append(val_auc)\n",
    "\n",
    "print(f\"Completed training {len(C_values)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nhzh3yyq2v",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the regularization path and model performance\n",
    "plt.close('all')  # Close previous figures to prevent lag\n",
    "\n",
    "coefficients_array = np.array(coefficients)\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 18))\n",
    "\n",
    "# Plot 1: Coefficient paths as lambda increases using seaborn\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Prepare data for seaborn lineplot\n",
    "coef_df = pd.DataFrame(coefficients_array, columns=feature_names)\n",
    "coef_df['lambda'] = lambdas\n",
    "\n",
    "# Melt for seaborn format\n",
    "coef_melted = coef_df.melt(id_vars=['lambda'], var_name='Feature', value_name='Coefficient')\n",
    "\n",
    "# Plot using seaborn\n",
    "sns.lineplot(data=coef_melted, x='lambda', y='Coefficient', hue='Feature', \n",
    "             ax=ax1, linewidth=2, alpha=0.6, legend=False)\n",
    "\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Lambda (Regularization Strength)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Coefficient Magnitude (Absolute Value)', fontsize=14, fontweight='bold')\n",
    "ax1.set_title('Lasso Regularization Path: Feature Selection for Incident Prediction', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "ax1.axhline(y=0, color='black', linestyle='-', linewidth=1.5)\n",
    "sns.despine(ax=ax1)\n",
    "\n",
    "# Annotate top features at the end\n",
    "final_coef_idx = np.argsort(coefficients_array[-1])[-8:][::-1]  # Top 8 features\n",
    "colors_palette = sns.color_palette(\"husl\", len(feature_names))\n",
    "for rank, idx in enumerate(final_coef_idx):\n",
    "    y_pos = coefficients_array[-1, idx]\n",
    "    ax1.annotate(feature_names[idx], \n",
    "                xy=(lambdas[-1], y_pos),\n",
    "                xytext=(10, 5 - rank*8),\n",
    "                textcoords='offset points',\n",
    "                fontsize=9,\n",
    "                fontweight='bold',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor=colors_palette[idx], alpha=0.7),\n",
    "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.3', \n",
    "                              color=colors_palette[idx], linewidth=1.5))\n",
    "\n",
    "# Plot 2: Model accuracy vs lambda\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Prepare data for seaborn\n",
    "accuracy_df = pd.DataFrame({\n",
    "    'lambda': np.concatenate([lambdas, lambdas]),\n",
    "    'Accuracy': np.concatenate([train_accuracies, val_accuracies]),\n",
    "    'Dataset': ['Training']*len(lambdas) + ['Validation']*len(lambdas)\n",
    "})\n",
    "\n",
    "sns.lineplot(data=accuracy_df, x='lambda', y='Accuracy', hue='Dataset', \n",
    "             style='Dataset', markers=['o', 's'], dashes=False, \n",
    "             linewidth=3, markersize=8, markevery=5, ax=ax2,\n",
    "             palette={'Training': '#3498db', 'Validation': '#e74c3c'})\n",
    "\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xlabel('Lambda (Regularization Strength)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.set_title('Model Performance vs Regularization Strength', fontsize=16, fontweight='bold', pad=20)\n",
    "sns.despine(ax=ax2)\n",
    "\n",
    "# Mark the best validation accuracy\n",
    "best_idx = np.argmax(val_accuracies)\n",
    "ax2.plot(lambdas[best_idx], val_accuracies[best_idx], marker='*', markersize=25, \n",
    "         color='#f39c12', markeredgecolor='black', markeredgewidth=2, zorder=10)\n",
    "\n",
    "ax2.annotate(f'Optimal Î» = {lambdas[best_idx]:.4f}\\nVal Acc = {val_accuracies[best_idx]:.4f}',\n",
    "            xy=(lambdas[best_idx], val_accuracies[best_idx]),\n",
    "            xytext=(30, -30),\n",
    "            textcoords='offset points',\n",
    "            fontsize=11,\n",
    "            fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='#f39c12', alpha=0.8, edgecolor='black', linewidth=2),\n",
    "            arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.3', \n",
    "                          color='black', linewidth=2))\n",
    "\n",
    "ax2.legend(fontsize=12, loc='lower left', framealpha=0.9, edgecolor='black')\n",
    "\n",
    "# Plot 3: F1 Score and AUC vs lambda\n",
    "ax3 = axes[2]\n",
    "\n",
    "metric_df = pd.DataFrame({\n",
    "    'lambda': np.concatenate([lambdas, lambdas]),\n",
    "    'Score': np.concatenate([val_f1_scores, val_auc_scores]),\n",
    "    'Metric': ['F1 Score']*len(lambdas) + ['AUC-ROC']*len(lambdas)\n",
    "})\n",
    "\n",
    "sns.lineplot(data=metric_df, x='lambda', y='Score', hue='Metric', \n",
    "             style='Metric', markers=['D', '^'], dashes=False, \n",
    "             linewidth=3, markersize=8, markevery=5, ax=ax3,\n",
    "             palette={'F1 Score': '#9b59b6', 'AUC-ROC': '#16a085'})\n",
    "\n",
    "ax3.set_xscale('log')\n",
    "ax3.set_xlabel('Lambda (Regularization Strength)', fontsize=14, fontweight='bold')\n",
    "ax3.set_ylabel('Score', fontsize=14, fontweight='bold')\n",
    "ax3.set_title('Additional Metrics: F1 Score and AUC-ROC', fontsize=16, fontweight='bold', pad=20)\n",
    "sns.despine(ax=ax3)\n",
    "ax3.legend(fontsize=12, loc='lower left', framealpha=0.9, edgecolor='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print beautiful summary of best model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"           ðŸŽ¯ OPTIMAL LASSO LOGISTIC REGRESSION MODEL ðŸŽ¯\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Best Validation Accuracy:':<30} {val_accuracies[best_idx]:.4f}\")\n",
    "print(f\"{'Best Validation F1 Score:':<30} {val_f1_scores[best_idx]:.4f}\")\n",
    "print(f\"{'Best Validation AUC-ROC:':<30} {val_auc_scores[best_idx]:.4f}\")\n",
    "print(f\"{'Optimal Lambda:':<30} {lambdas[best_idx]:.6f}\")\n",
    "print(f\"{'Optimal C (1/Lambda):':<30} {C_values[best_idx]:.6f}\")\n",
    "print(f\"{'Training Accuracy:':<30} {train_accuracies[best_idx]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"           ðŸ† TOP 10 MOST IMPORTANT FEATURES ðŸ†\")\n",
    "print(\"=\"*70)\n",
    "best_coefs = coefficients_array[best_idx]\n",
    "top_features_idx = np.argsort(best_coefs)[-10:][::-1]\n",
    "\n",
    "print(f\"\\n{'Rank':<6} {'Feature Name':<35} {'Coefficient Magnitude':<20}\")\n",
    "print(\"-\"*70)\n",
    "for rank, idx in enumerate(top_features_idx, 1):\n",
    "    print(f\"{rank:<6} {feature_names[idx]:<35} {best_coefs[idx]:<20.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y0mdnvcsvgs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with optimal hyperparameters and evaluate on test set\n",
    "plt.close('all')  # Close previous figures to prevent lag\n",
    "\n",
    "best_C = C_values[best_idx]\n",
    "\n",
    "final_model = LogisticRegression(\n",
    "    penalty='l1',\n",
    "    C=best_C,\n",
    "    solver='saga',\n",
    "    max_iter=2000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = final_model.predict(X_train_scaled)\n",
    "y_val_pred = final_model.predict(X_val_scaled)\n",
    "y_test_pred = final_model.predict(X_test_scaled)\n",
    "\n",
    "# Get probabilities for AUC\n",
    "y_train_probs = final_model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_val_probs = final_model.predict_proba(X_val_scaled)[:, 1]\n",
    "y_test_probs = final_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"     ðŸ“Š FINAL LASSO LOGISTIC REGRESSION MODEL PERFORMANCE ðŸ“Š\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Dataset':<15} {'Accuracy':<12} {'F1 Score':<12} {'AUC-ROC':<12} {'Samples':<10}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Training':<15} {accuracy_score(y_train, y_train_pred):<12.4f} {f1_score(y_train, y_train_pred):<12.4f} {roc_auc_score(y_train, y_train_probs):<12.4f} {len(y_train):<10}\")\n",
    "print(f\"{'Validation':<15} {accuracy_score(y_val, y_val_pred):<12.4f} {f1_score(y_val, y_val_pred):<12.4f} {roc_auc_score(y_val, y_val_probs):<12.4f} {len(y_val):<10}\")\n",
    "print(f\"{'Test':<15} {accuracy_score(y_test, y_test_pred):<12.4f} {f1_score(y_test, y_test_pred):<12.4f} {roc_auc_score(y_test, y_test_probs):<12.4f} {len(y_test):<10}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"               ðŸ“‹ CLASSIFICATION REPORT (Test Set) ðŸ“‹\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Beautiful confusion matrix using seaborn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu', cbar_kws={'label': 'Count'}, \n",
    "            linewidths=2, linecolor='white', square=True, ax=ax1,\n",
    "            annot_kws={'fontsize': 14, 'fontweight': 'bold'})\n",
    "\n",
    "ax1.set_title('Confusion Matrix - Test Set\\nIncident Occurred Prediction', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax1.set_xlabel('Predicted', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('True', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticklabels(['No Incident', 'Incident'])\n",
    "ax1.set_yticklabels(['No Incident', 'Incident'])\n",
    "\n",
    "# ROC Curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_probs)\n",
    "test_auc = roc_auc_score(y_test, y_test_probs)\n",
    "\n",
    "ax2.plot(fpr, tpr, linewidth=3, color='#3498db', label=f'ROC Curve (AUC = {test_auc:.4f})')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "ax2.set_xlabel('False Positive Rate', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('True Positive Rate', fontsize=13, fontweight='bold')\n",
    "ax2.set_title('ROC Curve - Test Set', fontsize=16, fontweight='bold', pad=20)\n",
    "ax2.legend(fontsize=12, framealpha=0.9, edgecolor='black')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "sns.despine(ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb7b2hu668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare normalization techniques on Logistic Regression\n",
    "plt.close('all')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"     ðŸ“Š NORMALIZATION TECHNIQUE COMPARISON ðŸ“Š\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "normalization_results = []\n",
    "\n",
    "for scaler_name, X_tr, X_v, X_te in [\n",
    "    ('StandardScaler', X_train_standard, X_val_standard, X_test_standard),\n",
    "    ('MinMaxScaler', X_train_minmax, X_val_minmax, X_test_minmax),\n",
    "    ('RobustScaler', X_train_robust, X_val_robust, X_test_robust)\n",
    "]:\n",
    "    # Train model with optimal C from previous analysis\n",
    "    model = LogisticRegression(\n",
    "        penalty='l1',\n",
    "        C=best_C if 'best_C' in dir() else 1.0,\n",
    "        solver='saga',\n",
    "        max_iter=2000,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_tr, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = accuracy_score(y_train, model.predict(X_tr))\n",
    "    val_acc = accuracy_score(y_val, model.predict(X_v))\n",
    "    test_acc = accuracy_score(y_test, model.predict(X_te))\n",
    "    \n",
    "    test_f1 = f1_score(y_test, model.predict(X_te))\n",
    "    test_auc = roc_auc_score(y_test, model.predict_proba(X_te)[:, 1])\n",
    "    \n",
    "    normalization_results.append({\n",
    "        'Scaler': scaler_name,\n",
    "        'Train': train_acc,\n",
    "        'Validation': val_acc,\n",
    "        'Test': test_acc,\n",
    "        'Test F1': test_f1,\n",
    "        'Test AUC': test_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{scaler_name}:\")\n",
    "    print(f\"  Train: {train_acc:.4f} | Val: {val_acc:.4f} | Test: {test_acc:.4f}\")\n",
    "    print(f\"  Test F1: {test_f1:.4f} | Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "# Create beautiful comparison plot\n",
    "results_df = pd.DataFrame(normalization_results)\n",
    "results_melted = results_df.melt(id_vars='Scaler', \n",
    "                                  value_vars=['Train', 'Validation', 'Test'],\n",
    "                                  var_name='Dataset', value_name='Accuracy')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.barplot(data=results_melted, x='Scaler', y='Accuracy', hue='Dataset', \n",
    "            palette={'Train': '#3498db', 'Validation': '#e74c3c', 'Test': '#2ecc71'},\n",
    "            ax=ax, edgecolor='black', linewidth=1.5, alpha=0.85)\n",
    "\n",
    "ax.set_title('Incident Prediction Performance Across Normalization Techniques', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Normalization Method', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy', fontsize=13, fontweight='bold')\n",
    "ax.set_ylim([0, 1])\n",
    "sns.despine()\n",
    "\n",
    "# Add value labels on bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.legend(title='Dataset', fontsize=11, title_fontsize=12, framealpha=0.9, edgecolor='black')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best scaler\n",
    "best_result = results_df.loc[results_df['Test'].idxmax()]\n",
    "print(f\"\\nðŸ† Best Normalization: {best_result['Scaler']} (Test Acc: {best_result['Test']:.4f})\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5hnxz583rj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Analysis with Different Kernels\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"     ðŸ¤– SUPPORT VECTOR MACHINE (SVM) ANALYSIS ðŸ¤–\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define kernels to test\n",
    "kernels = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "C_values_svm = np.logspace(-2, 2, 30)  # Regularization parameter\n",
    "\n",
    "# Store results for each kernel\n",
    "svm_results = {kernel: {'train': [], 'val': [], 'C_values': C_values_svm} for kernel in kernels}\n",
    "\n",
    "print(\"\\nTraining SVM models with different kernels...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "for kernel in kernels:\n",
    "    print(f\"Training {kernel.upper()} kernel SVM...\")\n",
    "    for C in C_values_svm:\n",
    "        # Use StandardScaler as it's typically best for SVM\n",
    "        model = SVC(\n",
    "            kernel=kernel,\n",
    "            C=C,\n",
    "            random_state=42,\n",
    "            degree=3 if kernel == 'poly' else 3,  # degree only matters for poly\n",
    "            gamma='scale'  # auto-adjust based on features\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train_standard, y_train)\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, model.predict(X_train_standard))\n",
    "        val_acc = accuracy_score(y_val, model.predict(X_val_standard))\n",
    "        \n",
    "        svm_results[kernel]['train'].append(train_acc)\n",
    "        svm_results[kernel]['val'].append(val_acc)\n",
    "    \n",
    "    # Find best C for this kernel\n",
    "    best_idx_kernel = np.argmax(svm_results[kernel]['val'])\n",
    "    best_C_kernel = C_values_svm[best_idx_kernel]\n",
    "    best_val_acc = svm_results[kernel]['val'][best_idx_kernel]\n",
    "    \n",
    "    print(f\"  âœ“ Best C: {best_C_kernel:.4f}, Val Acc: {best_val_acc:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Completed training all SVM models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3m02hpb7cy8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SVM Performance Across Different Kernels\n",
    "plt.close('all')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "kernel_colors = {\n",
    "    'linear': '#3498db',\n",
    "    'rbf': '#e74c3c', \n",
    "    'poly': '#2ecc71',\n",
    "    'sigmoid': '#f39c12'\n",
    "}\n",
    "\n",
    "# Plot each kernel in its own subplot\n",
    "for idx, kernel in enumerate(kernels):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Prepare data for seaborn\n",
    "    kernel_df = pd.DataFrame({\n",
    "        'C': np.concatenate([C_values_svm, C_values_svm]),\n",
    "        'Accuracy': np.concatenate([svm_results[kernel]['train'], svm_results[kernel]['val']]),\n",
    "        'Dataset': ['Training']*len(C_values_svm) + ['Validation']*len(C_values_svm)\n",
    "    })\n",
    "    \n",
    "    # Plot using seaborn\n",
    "    sns.lineplot(data=kernel_df, x='C', y='Accuracy', hue='Dataset', style='Dataset',\n",
    "                markers=['o', 's'], dashes=False, linewidth=3, markersize=6, \n",
    "                markevery=3, ax=ax,\n",
    "                palette={'Training': kernel_colors[kernel], \n",
    "                        'Validation': sns.light_palette(kernel_colors[kernel], n_colors=3)[0]})\n",
    "    \n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('C (Regularization Parameter)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{kernel.upper()} Kernel SVM - Incident Prediction', fontsize=14, fontweight='bold', pad=15)\n",
    "    sns.despine(ax=ax)\n",
    "    \n",
    "    # Mark best validation point\n",
    "    best_idx_k = np.argmax(svm_results[kernel]['val'])\n",
    "    best_C_k = C_values_svm[best_idx_k]\n",
    "    best_val_k = svm_results[kernel]['val'][best_idx_k]\n",
    "    \n",
    "    ax.plot(best_C_k, best_val_k, marker='*', markersize=20, \n",
    "            color='gold', markeredgecolor='black', markeredgewidth=2, zorder=10)\n",
    "    \n",
    "    ax.annotate(f'Best C={best_C_k:.3f}\\nVal Acc={best_val_k:.4f}',\n",
    "               xy=(best_C_k, best_val_k),\n",
    "               xytext=(20, -20),\n",
    "               textcoords='offset points',\n",
    "               fontsize=10,\n",
    "               fontweight='bold',\n",
    "               bbox=dict(boxstyle='round,pad=0.4', facecolor='gold', alpha=0.7, edgecolor='black', linewidth=1.5),\n",
    "               arrowprops=dict(arrowstyle='->', color='black', linewidth=1.5))\n",
    "    \n",
    "    ax.legend(fontsize=10, framealpha=0.9, edgecolor='black')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.suptitle('SVM Performance Comparison for Incident Prediction', \n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"           ðŸ“Š SVM KERNEL COMPARISON SUMMARY ðŸ“Š\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Kernel':<12} {'Best C':<12} {'Train Acc':<12} {'Val Acc':<12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for kernel in kernels:\n",
    "    best_idx_k = np.argmax(svm_results[kernel]['val'])\n",
    "    best_C_k = C_values_svm[best_idx_k]\n",
    "    train_acc_k = svm_results[kernel]['train'][best_idx_k]\n",
    "    val_acc_k = svm_results[kernel]['val'][best_idx_k]\n",
    "    \n",
    "    print(f\"{kernel:<12} {best_C_k:<12.4f} {train_acc_k:<12.4f} {val_acc_k:<12.4f}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de7e03del",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best SVM model and compare all kernels on test set\n",
    "plt.close('all')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"     ðŸ† FINAL SVM EVALUATION ON TEST SET ðŸ†\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "svm_test_results = []\n",
    "\n",
    "for kernel in kernels:\n",
    "    best_idx_k = np.argmax(svm_results[kernel]['val'])\n",
    "    best_C_k = C_values_svm[best_idx_k]\n",
    "    \n",
    "    # Train final model with best C\n",
    "    final_svm = SVC(\n",
    "        kernel=kernel,\n",
    "        C=best_C_k,\n",
    "        random_state=42,\n",
    "        degree=3,\n",
    "        gamma='scale',\n",
    "        probability=True  # Enable probability estimates for AUC\n",
    "    )\n",
    "    final_svm.fit(X_train_standard, y_train)\n",
    "    \n",
    "    # Evaluate on all sets\n",
    "    train_acc = accuracy_score(y_train, final_svm.predict(X_train_standard))\n",
    "    val_acc = accuracy_score(y_val, final_svm.predict(X_val_standard))\n",
    "    test_acc = accuracy_score(y_test, final_svm.predict(X_test_standard))\n",
    "    \n",
    "    test_f1 = f1_score(y_test, final_svm.predict(X_test_standard))\n",
    "    test_auc = roc_auc_score(y_test, final_svm.predict_proba(X_test_standard)[:, 1])\n",
    "    \n",
    "    svm_test_results.append({\n",
    "        'Kernel': kernel,\n",
    "        'C': best_C_k,\n",
    "        'Train': train_acc,\n",
    "        'Validation': val_acc,\n",
    "        'Test': test_acc,\n",
    "        'Test F1': test_f1,\n",
    "        'Test AUC': test_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{kernel.upper()} Kernel (C={best_C_k:.4f}):\")\n",
    "    print(f\"  Train: {train_acc:.4f} | Val: {val_acc:.4f} | Test: {test_acc:.4f}\")\n",
    "    print(f\"  Test F1: {test_f1:.4f} | Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "# Create beautiful comparison visualization\n",
    "svm_test_df = pd.DataFrame(svm_test_results)\n",
    "svm_test_melted = svm_test_df.melt(id_vars=['Kernel', 'C'], \n",
    "                                     value_vars=['Train', 'Validation', 'Test'],\n",
    "                                     var_name='Dataset', value_name='Accuracy')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Plot 1: Bar chart comparison\n",
    "sns.barplot(data=svm_test_melted, x='Kernel', y='Accuracy', hue='Dataset',\n",
    "           palette={'Train': '#3498db', 'Validation': '#e74c3c', 'Test': '#2ecc71'},\n",
    "           ax=ax1, edgecolor='black', linewidth=1.5, alpha=0.85)\n",
    "\n",
    "ax1.set_title('SVM Performance by Kernel - Incident Prediction', fontsize=16, fontweight='bold', pad=20)\n",
    "ax1.set_xlabel('Kernel', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.legend(title='Dataset', fontsize=11, title_fontsize=12, framealpha=0.9, edgecolor='black')\n",
    "sns.despine(ax=ax1)\n",
    "\n",
    "# Add value labels\n",
    "for container in ax1.containers:\n",
    "    ax1.bar_label(container, fmt='%.3f', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 2: Test accuracy comparison with sorted ranking\n",
    "test_only = svm_test_df.sort_values('Test', ascending=True)\n",
    "colors_ranked = [kernel_colors[k] for k in test_only['Kernel']]\n",
    "\n",
    "sns.barplot(data=test_only, y='Kernel', x='Test', palette=colors_ranked,\n",
    "           ax=ax2, edgecolor='black', linewidth=1.5, alpha=0.85)\n",
    "\n",
    "ax2.set_title('SVM Test Accuracy Ranking - Incident Prediction', fontsize=16, fontweight='bold', pad=20)\n",
    "ax2.set_xlabel('Test Accuracy', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('Kernel', fontsize=13, fontweight='bold')\n",
    "ax2.set_xlim([0, 1])\n",
    "sns.despine(ax=ax2)\n",
    "\n",
    "# Add value labels\n",
    "ax2.bar_label(ax2.containers[0], fmt='%.4f', fontsize=10, fontweight='bold', padding=5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"           ðŸ“‹ COMPLETE SVM RESULTS TABLE ðŸ“‹\")\n",
    "print(\"=\"*70)\n",
    "print(svm_test_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_kernel_idx = svm_test_df['Test'].idxmax()\n",
    "best_kernel = svm_test_df.loc[best_kernel_idx, 'Kernel']\n",
    "best_test_acc = svm_test_df.loc[best_kernel_idx, 'Test']\n",
    "\n",
    "print(f\"\\nðŸ¥‡ BEST SVM: {best_kernel.upper()} Kernel (Test Acc: {best_test_acc:.4f})\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hd1n7hryvsm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and ROC Curve for Best SVM Kernel\n",
    "plt.close('all')\n",
    "\n",
    "# Train best SVM model\n",
    "best_kernel_idx = svm_test_df['Test'].idxmax()\n",
    "best_svm_kernel = svm_test_df.loc[best_kernel_idx, 'Kernel']\n",
    "best_svm_C = svm_test_df.loc[best_kernel_idx, 'C']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"     ðŸŽ¯ BEST SVM MODEL: {best_svm_kernel.upper()} KERNEL ðŸŽ¯\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_svm_model = SVC(\n",
    "    kernel=best_svm_kernel,\n",
    "    C=best_svm_C,\n",
    "    random_state=42,\n",
    "    degree=3,\n",
    "    gamma='scale',\n",
    "    probability=True\n",
    ")\n",
    "best_svm_model.fit(X_train_standard, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_test_pred_svm = best_svm_model.predict(X_test_standard)\n",
    "y_test_probs_svm = best_svm_model.predict_proba(X_test_standard)[:, 1]\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"               ðŸ“‹ CLASSIFICATION REPORT (Test Set) ðŸ“‹\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_test, y_test_pred_svm))\n",
    "\n",
    "# Beautiful confusion matrix and ROC curve\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "cm_svm = confusion_matrix(y_test, y_test_pred_svm)\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='RdPu', cbar_kws={'label': 'Count'}, \n",
    "            linewidths=2, linecolor='white', square=True, ax=ax1,\n",
    "            annot_kws={'fontsize': 14, 'fontweight': 'bold'})\n",
    "\n",
    "ax1.set_title(f'Confusion Matrix\\n{best_svm_kernel.upper()} Kernel SVM (C={best_svm_C:.4f})', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax1.set_xlabel('Predicted', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('True', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticklabels(['No Incident', 'Incident'])\n",
    "ax1.set_yticklabels(['No Incident', 'Incident'])\n",
    "\n",
    "# ROC Curve\n",
    "fpr_svm, tpr_svm, _ = roc_curve(y_test, y_test_probs_svm)\n",
    "test_auc_svm = roc_auc_score(y_test, y_test_probs_svm)\n",
    "\n",
    "ax2.plot(fpr_svm, tpr_svm, linewidth=3, color='#9b59b6', label=f'ROC Curve (AUC = {test_auc_svm:.4f})')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "ax2.set_xlabel('False Positive Rate', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('True Positive Rate', fontsize=13, fontweight='bold')\n",
    "ax2.set_title('ROC Curve - Best SVM', fontsize=16, fontweight='bold', pad=20)\n",
    "ax2.legend(fontsize=12, framealpha=0.9, edgecolor='black')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "sns.despine(ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "test_acc_svm = accuracy_score(y_test, y_test_pred_svm)\n",
    "print(f\"\\nOverall Test Accuracy: {test_acc_svm:.4f}\")\n",
    "print(f\"Test F1 Score: {f1_score(y_test, y_test_pred_svm):.4f}\")\n",
    "print(f\"Test AUC-ROC: {test_auc_svm:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
